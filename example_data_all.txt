================================================================================
EXAMPLE 1: Mixed Quality - Machine Learning Topic
================================================================================
Description: Mixed quality retrieval with some relevant, some redundant, and 
some irrelevant chunks. Good for testing all three evaluators.

QUERY:
What is the difference between supervised and unsupervised learning in machine learning?

CHUNKS:
Supervised learning is a type of machine learning where the algorithm learns from labeled training data. The model is trained on input-output pairs, where the correct output (label) is provided for each input example. Common examples include classification tasks like spam detection and regression tasks like predicting house prices. The key advantage is that the model can learn patterns directly from the labeled examples.

Unsupervised learning involves training models on data without labeled examples. The algorithm must find hidden patterns or structures in the data on its own. Common techniques include clustering (grouping similar data points) and dimensionality reduction (reducing the number of features). Unlike supervised learning, there are no correct answers provided during training.

Supervised learning algorithms require labeled datasets where each example has a known output. Popular algorithms include linear regression, decision trees, random forests, and neural networks. The training process involves minimizing the error between predicted and actual outputs. This approach works well when you have sufficient labeled data and clear objectives.

Unsupervised learning is useful when you don't have labeled data or want to discover unknown patterns. K-means clustering groups data into clusters based on similarity, while principal component analysis (PCA) reduces dimensionality. These methods are valuable for exploratory data analysis and can reveal insights that weren't initially obvious.

Python is a high-level programming language known for its simplicity and readability. It was created by Guido van Rossum and first released in 1991. Python supports multiple programming paradigms including procedural, object-oriented, and functional programming. It has a large standard library and is widely used in web development, data science, and automation.


================================================================================
EXAMPLE 2: High Redundancy Scenario - Climate Change Topic
================================================================================
Description: Multiple chunks with very similar content. Should trigger high 
redundancy warnings. All chunks are relevant but highly redundant.

QUERY:
What are the main causes of climate change?

CHUNKS:
Climate change is primarily caused by greenhouse gas emissions from human activities. The main sources include burning fossil fuels for energy, deforestation, industrial processes, and agricultural practices. Carbon dioxide is the most significant greenhouse gas, accounting for about 76% of total emissions.

The primary drivers of climate change are greenhouse gases released through human activities. Key contributors are fossil fuel combustion for power generation, forest clearing, manufacturing operations, and farming methods. CO2 represents the largest portion of greenhouse gas emissions, making up approximately three-quarters of the total.

Human activities that release greenhouse gases are the main causes of climate change. These activities include using coal, oil, and gas for electricity and transportation, cutting down forests, and various industrial and agricultural processes. Carbon dioxide emissions are the dominant factor, representing around 76% of all greenhouse gas emissions.

Fossil fuel burning is the largest source of greenhouse gas emissions causing climate change. When we burn coal, oil, and natural gas for energy, we release carbon dioxide into the atmosphere. Deforestation also contributes significantly by reducing the number of trees that can absorb CO2.

The greenhouse effect is enhanced by human activities, leading to global warming. Industrial processes, transportation, and energy production from fossil fuels are major contributors. Methane from agriculture and landfills also plays a significant role, though CO2 remains the primary greenhouse gas.


================================================================================
EXAMPLE 3: Low Relevance (Retrieval Failure) - Quantum Computing Topic
================================================================================
Description: Query about quantum computing, but chunks are mostly about 
classical computing and general physics. Should trigger missing context warnings.

QUERY:
How do quantum computers use qubits to perform calculations?

CHUNKS:
Classical computers use bits that can be either 0 or 1. These bits are processed by transistors in integrated circuits. The CPU executes instructions sequentially, performing calculations using binary arithmetic. Modern processors can execute billions of operations per second.

Quantum mechanics describes the behavior of particles at the atomic and subatomic level. Particles can exist in superposition states, meaning they can be in multiple states simultaneously until measured. This probabilistic nature is fundamental to quantum physics.

The first electronic computers were developed in the 1940s. ENIAC, completed in 1945, was one of the earliest general-purpose computers. It used vacuum tubes and could perform calculations much faster than mechanical calculators. Modern computers have evolved significantly from these early machines.

Binary number systems use only two digits, 0 and 1. This base-2 system is fundamental to digital computing. All data in computers is ultimately represented as binary digits. Boolean algebra provides the mathematical foundation for binary operations.

Computer algorithms are step-by-step procedures for solving problems. They can be implemented in various programming languages. Algorithm efficiency is measured by time and space complexity. Common algorithms include sorting, searching, and graph traversal techniques.


================================================================================
EXAMPLE 4: Good Quality Retrieval - Neural Networks Topic
================================================================================
Description: All chunks are highly relevant with good coverage of different 
aspects. Low redundancy with complementary information. Should show good 
scores across all metrics.

QUERY:
What are the key components of a neural network and how do they work together?

CHUNKS:
Neural networks consist of interconnected nodes called neurons, organized into layers. The input layer receives data, hidden layers process information, and the output layer produces results. Each neuron receives inputs, applies weights, and passes the result through an activation function.

Connections between neurons have associated weights that determine signal strength. During training, these weights are adjusted to minimize prediction errors. The learning process uses algorithms like backpropagation to update weights based on the difference between predicted and actual outputs.

Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns. Common activation functions include ReLU (Rectified Linear Unit), sigmoid, and tanh. These functions determine whether a neuron should be activated based on its input.

Forward propagation passes input data through the network layer by layer to generate predictions. Backward propagation then calculates gradients and updates weights to improve accuracy. This iterative process continues until the model converges to an optimal solution.

Loss functions measure how far predictions are from actual values. Common loss functions include mean squared error for regression and cross-entropy for classification. Optimization algorithms like gradient descent adjust weights to minimize the loss function.
